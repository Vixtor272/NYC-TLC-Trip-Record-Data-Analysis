{"block_file": {"custom/trigger.py:custom:python:trigger": {"content": "# trigger_sequential_controller_custom_safe.py\nif 'custom' not in globals():\n    from mage_ai.data_preparation.decorators import custom\nfrom mage_ai.orchestration.triggers.api import trigger_pipeline\nfrom datetime import datetime\nimport time\nimport traceback\n\n# CONFIG: pon el UUID del pipeline destino (NO el del controller)\nDEST_PIPELINE_UUID = 'ny_taxi_pipeline'  \nSTART_YEAR = 2015\nEND_YEAR = 2025\nEND_YEAR_LAST_MONTH = 12\nSERVICE = 'yellow'\nSLEEP_BETWEEN_TRIGGERS = 5\nPOLL_INTERVAL = 10\nPOLL_TIMEOUT = 300\n\n@custom\ndef run(*args, **kwargs):\n    # overrides desde UI si quieres\n    dest = kwargs.get('dest_pipeline_uuid') or DEST_PIPELINE_UUID\n    start = int(kwargs.get('start_year') or START_YEAR)\n    end = int(kwargs.get('end_year') or END_YEAR)\n    end_month = int(kwargs.get('end_year_last_month') or END_YEAR_LAST_MONTH)\n    service = kwargs.get('service') or SERVICE\n\n    # PREVENCI\u00d3N: no disparar el mismo pipeline (auto-trigger)\n    current_pipeline_uuid = kwargs.get('current_pipeline_uuid')\n    if current_pipeline_uuid and current_pipeline_uuid == dest:\n        raise RuntimeError(\"DEST_PIPELINE_UUID es el mismo que el pipeline controlador. No permitido.\")\n\n    try:\n        for y in range(start, end + 1):\n            months = range(1, end_month + 1) if y == end else range(1, 13)\n            for m in months:\n                print(f\"{datetime.now().isoformat()} - Triggering {dest} for {y}-{m:02d} service={service}\")\n                try:\n                    info = trigger_pipeline(\n                        dest,\n                        variables={'year': y, 'month': m, 'service': service},\n                        check_status=True,\n                        error_on_failure=True,\n                        poll_interval=POLL_INTERVAL,\n                        poll_timeout=POLL_TIMEOUT,\n                        verbose=True,\n                    )\n                    # info suele contener detalles del run; m\u00f3stralo\n                    print(\"Trigger result:\", info)\n                except Exception as e:\n                    print(f\"[ERROR] Al disparar {y}-{m:02d}: {e}\")\n                    traceback.print_exc()\n                    # no abortar todo; continuar con siguiente mes\n                finally:\n                    time.sleep(SLEEP_BETWEEN_TRIGGERS)\n    except KeyboardInterrupt:\n        print(\"Interrumpido por usuario.\")", "file_path": "custom/trigger.py", "language": "python", "type": "custom", "uuid": "trigger"}, "data_exporters/exporter_taxi_yellow_trips.py:data_exporter:python:exporter taxi yellow trips": {"content": "# export_data_to_snowflake.py\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\nfrom mage_ai.io.snowflake import Snowflake\nimport pandas as pd\nimport uuid\nfrom datetime import datetime, timezone\n\n@data_exporter\ndef export_data_to_snowflake(df: pd.DataFrame, *args, **kwargs) -> None:\n    # Par\u00e1metros de entrada / defaults\n    year = int(kwargs.get('year', 2015))\n    month = int(kwargs.get('month', 5))\n    service = str(kwargs.get('service', 'yellow')).upper()\n\n    # normalizar a MAY\u00daSCULAS primero\n    df.columns = [str(c).upper() for c in df.columns]\n    \n    # Get secrets using get_secret_value\n    snowflake_config = {\n        'account': get_secret_value('SNOWFLAKE_ACCOUNT'),\n        'user': get_secret_value('SNOWFLAKE_USER'),\n        'password': get_secret_value('SNOWFLAKE_PASSWORD'),\n        'warehouse': get_secret_value('SNOWFLAKE_WH'),\n    }\n    \n    # Verify all secrets were retrieved\n    for key, value in snowflake_config.items():\n        if value is None:\n            raise ValueError(f\"Secret '{key}' not found in Mage secrets\")\n    \n    # Parametrizaci\u00f3n tabla\n    table_name = f\"{service}_TRIPS\"\n    database = \"NYC_TLC\"\n    schema = \"BRONZE\"\n    fecha_extraida = f\"{month:02d}-{year}\"\n\n    # Export using direct Snowflake connection\n    with Snowflake(\n        account=snowflake_config['account'],\n        user=snowflake_config['user'],\n        password=snowflake_config['password'],\n        warehouse=snowflake_config['warehouse'],\n        database=database,\n        schema=schema\n    ) as loader:\n        result = loader.execute(f\"\"\"\n            SELECT COUNT(*) \n            FROM {database}.{schema}.{table_name}\n            WHERE \"YEAR\" = {year} AND \"MONTH\" = {month}\n        \"\"\")\n        if result[0][0] == 0:\n            loader.export(\n                df,  # Use the modified DataFrame\n                table_name,\n                database,\n                schema,\n                if_exists='append',\n            )\n            print(f\"\\n Carga realizada: {len(df)} filas insertadas en {database}.{schema}.{table_name} para fecha {fecha_extraida}.\")\n        else:\n            print(f\"\\n Datos para YEAR={year} y MONTH={month} ya existen. No se realizar\u00e1 la carga.\")", "file_path": "data_exporters/exporter_taxi_yellow_trips.py", "language": "python", "type": "data_exporter", "uuid": "exporter_taxi_yellow_trips"}, "data_loaders/ingest_taxi_trips.py:data_loader:python:ingest taxi trips": {"content": "# data_loader.py\nimport pandas as pd\nif 'data_loader' not in globals(): \n    from mage_ai.data_preparation.decorators import data_loader \nif 'test' not in globals(): \n    from mage_ai.data_preparation.decorators import test \n\n@data_loader\ndef load_data_from_file(*args, **kwargs):\n    \"\"\"\n    Carga un archivo parquet remoto con los datos de taxi.\n    \"\"\"\n    year = int(kwargs.get('year', 2015))\n    month = int(kwargs.get('month', 5))\n    service = str(kwargs.get('service', 'yellow'))\n    url = f'https://d37ci6vzurychx.cloudfront.net/trip-data/{service}_tripdata_{year}-{month:02d}.parquet'\n    print('Iniciando descarga')\n    df = pd.read_parquet(url)  # requiere pyarrow/fastparquet instalado\n    print(f'Data del mes {month}-{year}-{service} descargada')\n    return df\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "data_loaders/ingest_taxi_trips.py", "language": "python", "type": "data_loader", "uuid": "ingest_taxi_trips"}, "data_loaders/ingest_zones.py:data_loader:python:ingest zones": {"content": "import pandas as pd\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\nimport snowflake.connector\nfrom snowflake.connector.pandas_tools import write_pandas\n\n\n@data_loader\ndef load_data(*args, **kwargs):\n    # URL del CSV (raw)\n    url = \"https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv\"\n\n    # Leer tal cual\n    df = pd.read_csv(url)\n\n    # --- Aqu\u00ed: poner columnas en MAY\u00daSCULAS ---\n    df.columns = [c.strip().upper() for c in df.columns]\n\n    # Credenciales desde secrets de Mage (crea estos secrets en tu proyecto)\n    user = get_secret_value('SNOWFLAKE_USER')\n    password = get_secret_value('SNOWFLAKE_PASSWORD')\n    account = get_secret_value('SNOWFLAKE_ACCOUNT')\n    role = get_secret_value('SNOWFLAKE_ROLE')\n\n    # Conexi\u00f3n m\u00ednima a Snowflake (usa la BD/SCHEMA que mencionaste)\n    conn = snowflake.connector.connect(\n        user=user,\n        password=password,\n        account=account,\n        role=role,\n        warehouse='COMPUTE_WH',\n        database='NYC_TLC',\n        schema='BRONZE',\n    )\n\n    # Subir tal cual; write_pandas crear\u00e1 la tabla si no existe\n    write_pandas(conn, df, 'TAXI_ZONES', database='NYC_TLC', schema='BRONZE')\n\n    conn.close()\n    return df\n\n\n@test\ndef test_output(output, *args) -> None:\n    # test m\u00ednimo obligatorio\n    assert output is not None, \"Output es None\"\n\n\n", "file_path": "data_loaders/ingest_zones.py", "language": "python", "type": "data_loader", "uuid": "ingest_zones"}, "data_loaders/trigger_sequential_controller.py:data_loader:python:trigger sequential controller": {"content": "# block: trigger_sequential_controller (Python block in Mage)\nfrom mage_ai.orchestration.triggers.api import trigger_pipeline\nfrom datetime import datetime\nimport time\nimport traceback\n\n# CONFIG (ajusta aqu\u00ed si quieres)\nDEST_PIPELINE_UUID = 'd3772e9521df436781187e6d93c2e2ac'   # reemplaza por el UUID del pipeline destino\nSTART_YEAR = 2015\nEND_YEAR = 2025\nEND_YEAR_LAST_MONTH = 8  # si END_YEAR tiene que llegar solo hasta agosto (por ejemplo 2025 \u2192 1..8). Pon 12 para todo el a\u00f1o.\nSERVICE = 'yellow'       # 'yellow' o 'green'\nSLEEP_BETWEEN_TRIGGERS = 10   # segundos entre disparos (despu\u00e9s de que termine el run)\nPOLL_INTERVAL = 10       # segundos entre checks internos de trigger_pipeline\nPOLL_TIMEOUT = 60 * 30   # timeout para esperar que termine cada run (segundos)\n\ndef run_controller(\n    dest_pipeline_uuid: str = DEST_PIPELINE_UUID,\n    start_year: int = START_YEAR,\n    end_year: int = END_YEAR,\n    end_year_last_month: int = END_YEAR_LAST_MONTH,\n    service: str = SERVICE,\n):\n    \"\"\"\n    Controlador que dispara el pipeline destino mes-a-mes en el rango especificado.\n    Espera a que cada run termine antes de disparar el siguiente (check_status=True).\n    \"\"\"\n    if not dest_pipeline_uuid:\n        raise ValueError(\"Define DEST_PIPELINE_UUID con el UUID del pipeline destino.\")\n\n    for year in range(start_year, end_year + 1):\n        months = range(1, end_year_last_month + 1) if year == end_year else range(1, 13)\n        for month in months:\n            try:\n                print(f\"{datetime.now().isoformat()} - Triggering {dest_pipeline_uuid} for {year}-{month:02d} service={service}\")\n                # trigger_pipeline espera y retorna info del run cuando check_status=True\n                info = trigger_pipeline(\n                    dest_pipeline_uuid,\n                    variables={'year': year, 'month': month, 'service': service},\n                    check_status=True,         # esperamos a que termine antes de seguir\n                    error_on_failure=True,     # lanza excepci\u00f3n si el run termina en failed\n                    poll_interval=POLL_INTERVAL,\n                    poll_timeout=POLL_TIMEOUT,\n                    verbose=True,\n                )\n                # info suele contener detalles del pipeline_run; imprime para debug\n                print(\"Trigger result:\", info)\n            except Exception as e:\n                # No detenemos todo: registramos y seguimos con el siguiente mes\n                print(f\"[ERROR] Al disparar {year}-{month:02d}: {e}\")\n                traceback.print_exc()\n            finally:\n                # Pausa corta para no sobrecargar la orquestaci\u00f3n (ajusta si quieres)\n                time.sleep(SLEEP_BETWEEN_TRIGGERS)\n\n# Si Mage espera una funci\u00f3n llamada 'run' en un bloque Python, exp\u00f3nla:\ndef run(*args, **kwargs):\n    # Puedes pasar override v\u00eda variables del block en la UI si prefieres\n    run_controller(\n        dest_pipeline_uuid=kwargs.get('dest_pipeline_uuid') or DEST_PIPELINE_UUID,\n        start_year=int(kwargs.get('start_year') or START_YEAR),\n        end_year=int(kwargs.get('end_year') or END_YEAR),\n        end_year_last_month=int(kwargs.get('end_year_last_month') or END_YEAR_LAST_MONTH),\n        service=kwargs.get('service') or SERVICE,\n    )\n\n", "file_path": "data_loaders/trigger_sequential_controller.py", "language": "python", "type": "data_loader", "uuid": "trigger_sequential_controller"}, "transformers/metadata_transform.py:transformer:python:metadata transform": {"content": "# add_metadata.py\nfrom datetime import datetime, timezone\nimport uuid\nimport pandas as pd\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n@transformer\ndef add_metadata(df: pd.DataFrame, *args, **kwargs) -> pd.DataFrame:\n    \"\"\"\n    Adds metadata columns to the taxi trips DataFrame.\n    \"\"\"\n\n    year = int(kwargs.get('year', 2015))\n    month = int(kwargs.get('month', 5))\n    service = str(kwargs.get('service', 'yellow')).upper()\n\n    ingest_run_id = str(uuid.uuid4())\n    ingest_timestamp = datetime.now(timezone.utc).replace(microsecond=0).isoformat()\n\n    # calculamos tama\u00f1o total en bytes (n\u00famero entero)\n    total_size_bytes = int(df.memory_usage(deep=True).sum())\n\n    # A\u00f1adimos columnas de metadata (tipos simples y serializables)\n    df['ingest_timestamp'] = ingest_timestamp  \n\n    df['size'] = total_size_bytes\n\n    df['ingest_run_id'] = ingest_run_id\n    df['ingest_run_id'] = df['ingest_run_id'].astype('category')\n\n    df['service'] = service\n    df['service'] = df['service'].astype('category')\n\n    # year/month son ints peque\u00f1os; si quieres ahorrar a\u00fan m\u00e1s:\n    df['year'] = year\n    df['year'] = df['year'].astype('int16')   # si rango lo permite\n    df['month'] = month\n    df['month'] = df['month'].astype('int8')\n\n\n    print(\"Metadata a\u00f1adida.\")\n\n    return df\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Test m\u00e1s expl\u00edcito: comprobamos que sea DataFrame y tenga filas.\n    \"\"\"\n    assert output is not None, 'El output es None'\n    assert isinstance(output, pd.DataFrame), f'Output no es DataFrame, es {type(output)}'\n    assert len(output) >= 0, 'Output tiene longitud negativa (imposible)'\n", "file_path": "transformers/metadata_transform.py", "language": "python", "type": "transformer", "uuid": "metadata_transform"}, "pipelines/ny_taxi_modelling/metadata.yaml:pipeline:yaml:ny taxi modelling/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    dbt_project_name: dbt/ny_taxi_modelling\n    file_path: dbt/ny_taxi_modelling/models/staging/stg_taxi_union.sql\n    file_source:\n      path: dbt/ny_taxi_modelling/models/staging/stg_taxi_union.sql\n      project_path: dbt/ny_taxi_modelling\n    limit: 1000\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: sql\n  name: dbt/ny_taxi_modelling/models/staging/stg_taxi_union\n  retry_config: null\n  status: updated\n  timeout: null\n  type: dbt\n  upstream_blocks: []\n  uuid: dbt/ny_taxi_modelling/models/staging/stg_taxi_union\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2025-09-27 01:13:06.483981+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: ny_taxi_modelling\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: ny_taxi_modelling\nvariables_dir: /home/src/mage_data/default_repo\nwidgets: []\n", "file_path": "pipelines/ny_taxi_modelling/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "ny_taxi_modelling/metadata"}, "pipelines/ny_taxi_modelling/__init__.py:pipeline:python:ny taxi modelling/  init  ": {"content": "", "file_path": "pipelines/ny_taxi_modelling/__init__.py", "language": "python", "type": "pipeline", "uuid": "ny_taxi_modelling/__init__"}, "pipelines/ny_taxi_pipeline/metadata.yaml:pipeline:yaml:ny taxi pipeline/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_source:\n      path: data_loaders/ingest_taxi_trips.py\n  downstream_blocks:\n  - metadata_transform\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: ingest_taxi_trips\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: ingest_taxi_trips\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_source:\n      path: transformers/metadata_transform.py\n  downstream_blocks:\n  - exporter_taxi_yellow_trips\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: metadata_transform\n  retry_config: null\n  status: updated\n  timeout: null\n  type: transformer\n  upstream_blocks:\n  - ingest_taxi_trips\n  uuid: metadata_transform\n- all_upstream_blocks_executed: false\n  color: null\n  configuration:\n    file_source:\n      path: data_exporters/exporter_taxi_yellow_trips.py\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: exporter_taxi_yellow_trips\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - metadata_transform\n  uuid: exporter_taxi_yellow_trips\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2025-09-23 17:38:49.917718+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: ny_taxi_pipeline\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: ny_taxi_pipeline\nvariables_dir: /home/src/mage_data/default_repo\nwidgets: []\n", "file_path": "pipelines/ny_taxi_pipeline/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "ny_taxi_pipeline/metadata"}, "pipelines/ny_taxi_pipeline/__init__.py:pipeline:python:ny taxi pipeline/  init  ": {"content": "", "file_path": "pipelines/ny_taxi_pipeline/__init__.py", "language": "python", "type": "pipeline", "uuid": "ny_taxi_pipeline/__init__"}, "pipelines/ny_taxi_zones/metadata.yaml:pipeline:yaml:ny taxi zones/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_source:\n      path: data_loaders/ingest_zones.py\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: ingest_zones\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: ingest_zones\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2025-09-27 04:12:39.684556+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: ny_taxi_zones\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: ny_taxi_zones\nvariables_dir: /home/src/mage_data/default_repo\nwidgets: []\n", "file_path": "pipelines/ny_taxi_zones/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "ny_taxi_zones/metadata"}, "pipelines/ny_taxi_zones/__init__.py:pipeline:python:ny taxi zones/  init  ": {"content": "", "file_path": "pipelines/ny_taxi_zones/__init__.py", "language": "python", "type": "pipeline", "uuid": "ny_taxi_zones/__init__"}, "pipelines/trigger_pipeline/metadata.yaml:pipeline:yaml:trigger pipeline/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: teal\n  configuration:\n    file_source:\n      path: custom/trigger.py\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: trigger\n  retry_config: null\n  status: updated\n  timeout: null\n  type: custom\n  upstream_blocks: []\n  uuid: trigger\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2025-09-30 04:43:16.630491+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: local_python\nextensions: {}\nname: trigger_pipeline\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: trigger_pipeline\nvariables_dir: /home/src/mage_data/default_repo\nwidgets: []\n", "file_path": "pipelines/trigger_pipeline/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "trigger_pipeline/metadata"}, "pipelines/trigger_pipeline/__init__.py:pipeline:python:trigger pipeline/  init  ": {"content": "", "file_path": "pipelines/trigger_pipeline/__init__.py", "language": "python", "type": "pipeline", "uuid": "trigger_pipeline/__init__"}}, "custom_block_template": {}, "mage_template": {"data_loaders/airtable.py:data_loader:python:Airtable:Load a Table from Airtable App.": {"block_type": "data_loader", "description": "Load a Table from Airtable App.", "language": "python", "name": "Airtable", "path": "data_loaders/airtable.py"}, "data_loaders/deltalake/s3.py:data_loader:python:Amazon S3:Load a Delta Table from Amazon S3.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_loaders/deltalake/s3.py"}, "data_loaders/deltalake/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Load a Delta Table from Azure Blob Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/deltalake/azure_blob_storage.py"}, "data_loaders/deltalake/gcs.py:data_loader:python:Google Cloud Storage:Load a Delta Table from Google Cloud Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/deltalake/gcs.py"}, "data_loaders/mongodb.py:data_loader:python:MongoDB:Load data from MongoDB.:Databases (NoSQL)": {"block_type": "data_loader", "description": "Load data from MongoDB.", "groups": ["Databases (NoSQL)"], "language": "python", "name": "MongoDB", "path": "data_loaders/mongodb.py"}, "data_loaders/mssql.py:data_loader:python:MSSQL:Load data from MSSQL.:Databases": {"block_type": "data_loader", "description": "Load data from MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_loaders/mssql.py"}, "data_exporters/deltalake/s3.py:data_exporter:python:Amazon S3:Export data to a Delta Table in Amazon S3.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_exporters/deltalake/s3.py"}, "data_exporters/deltalake/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Export data to a Delta Table in Azure Blob Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/deltalake/azure_blob_storage.py"}, "data_exporters/deltalake/gcs.py:data_exporter:python:Google Cloud Storage:Export data to a Delta Table in Google Cloud Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/deltalake/gcs.py"}, "data_exporters/mongodb.py:data_exporter:python:MongoDB:Export data to MongoDB.": {"block_type": "data_exporter", "description": "Export data to MongoDB.", "language": "python", "name": "MongoDB", "path": "data_exporters/mongodb.py"}, "data_exporters/mssql.py:data_exporter:python:MSSQL:Export data to MSSQL.:Databases": {"block_type": "data_exporter", "description": "Export data to MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_exporters/mssql.py"}, "data_loaders/orchestration/triggers/default.jinja:data_loader:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_loader", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_loaders/orchestration/triggers/default.jinja"}, "data_exporters/orchestration/triggers/default.jinja:data_exporter:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_exporter", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_exporters/orchestration/triggers/default.jinja"}, "callbacks/base.jinja:callback:python:Base template:Base template with empty functions.": {"block_type": "callback", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "callbacks/base.jinja"}, "callbacks/orchestration/triggers/default.jinja:callback:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "callback", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "callbacks/orchestration/triggers/default.jinja"}, "conditionals/base.jinja:conditional:python:Base template:Base template with empty functions.": {"block_type": "conditional", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "conditionals/base.jinja"}, "data_loaders/default.jinja:data_loader:python:Base template (generic)": {"block_type": "data_loader", "language": "python", "name": "Base template (generic)", "path": "data_loaders/default.jinja"}, "data_loaders/s3.py:data_loader:python:Amazon S3:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_loaders/s3.py"}, "data_loaders/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/azure_blob_storage.py"}, "data_loaders/google_cloud_storage.py:data_loader:python:Google Cloud Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/google_cloud_storage.py"}, "data_loaders/redshift.py:data_loader:python:Amazon Redshift:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_loaders/redshift.py"}, "data_loaders/bigquery.py:data_loader:python:Google BigQuery:Load data from Google BigQuery.:Data warehouses": {"block_type": "data_loader", "description": "Load data from Google BigQuery.", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_loaders/bigquery.py"}, "data_loaders/snowflake.py:data_loader:python:Snowflake:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_loaders/snowflake.py"}, "data_loaders/algolia.py:data_loader:python:Algolia:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_loaders/algolia.py"}, "data_loaders/chroma.py:data_loader:python:Chroma:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_loaders/chroma.py"}, "data_loaders/duckdb.py:data_loader:python:DuckDB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_loaders/duckdb.py"}, "data_loaders/mysql.py:data_loader:python:MySQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_loaders/mysql.py"}, "data_loaders/oracledb.py:data_loader:python:Oracle DB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Oracle DB", "path": "data_loaders/oracledb.py"}, "data_loaders/postgres.py:data_loader:python:PostgreSQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_loaders/postgres.py"}, "data_loaders/qdrant.py:data_loader:python:Qdrant:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_loaders/qdrant.py"}, "data_loaders/weaviate.py:data_loader:python:Weaviate:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_loaders/weaviate.py"}, "data_loaders/api.py:data_loader:python:API:Fetch data from an API request.": {"block_type": "data_loader", "description": "Fetch data from an API request.", "language": "python", "name": "API", "path": "data_loaders/api.py"}, "data_loaders/file.py:data_loader:python:Local file:Load data from a file on your machine.": {"block_type": "data_loader", "description": "Load data from a file on your machine.", "language": "python", "name": "Local file", "path": "data_loaders/file.py"}, "data_loaders/google_sheets.py:data_loader:python:Google Sheets:Load data from a worksheet in Google Sheets.": {"block_type": "data_loader", "description": "Load data from a worksheet in Google Sheets.", "language": "python", "name": "Google Sheets", "path": "data_loaders/google_sheets.py"}, "data_loaders/druid.py:data_loader:python:Druid": {"block_type": "data_loader", "language": "python", "name": "Druid", "path": "data_loaders/druid.py"}, "transformers/default.jinja:transformer:python:Base template (generic)": {"block_type": "transformer", "language": "python", "name": "Base template (generic)", "path": "transformers/default.jinja"}, "transformers/data_warehouse_transformer.jinja:transformer:python:Amazon Redshift:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "redshift", "data_source_handler": "Redshift"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Google BigQuery:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "", "data_source": "bigquery", "data_source_handler": "BigQuery"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Snowflake:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "snowflake", "data_source_handler": "Snowflake"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:PostgreSQL:Databases": {"block_type": "transformer", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "postgres", "data_source_handler": "Postgres"}}, "transformers/transformer_actions/row/drop_duplicate.py:transformer:python:Drop duplicate rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Drop duplicate rows", "path": "transformers/transformer_actions/row/drop_duplicate.py"}, "transformers/transformer_actions/row/filter.py:transformer:python:Filter rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Filter rows", "path": "transformers/transformer_actions/row/filter.py"}, "transformers/transformer_actions/row/remove.py:transformer:python:Remove rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Remove rows", "path": "transformers/transformer_actions/row/remove.py"}, "transformers/transformer_actions/row/sort.py:transformer:python:Sort rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Sort rows", "path": "transformers/transformer_actions/row/sort.py"}, "transformers/transformer_actions/column/average.py:transformer:python:Average value of column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Average value of column", "path": "transformers/transformer_actions/column/average.py"}, "transformers/transformer_actions/column/count_distinct.py:transformer:python:Count unique values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Count unique values in column", "path": "transformers/transformer_actions/column/count_distinct.py"}, "transformers/transformer_actions/column/first.py:transformer:python:First value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "First value in column", "path": "transformers/transformer_actions/column/first.py"}, "transformers/transformer_actions/column/last.py:transformer:python:Last value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Last value in column", "path": "transformers/transformer_actions/column/last.py"}, "transformers/transformer_actions/column/max.py:transformer:python:Maximum value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Maximum value in column", "path": "transformers/transformer_actions/column/max.py"}, "transformers/transformer_actions/column/median.py:transformer:python:Median value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Median value in column", "path": "transformers/transformer_actions/column/median.py"}, "transformers/transformer_actions/column/min.py:transformer:python:Min value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Min value in column", "path": "transformers/transformer_actions/column/min.py"}, "transformers/transformer_actions/column/sum.py:transformer:python:Sum of all values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Sum of all values in column", "path": "transformers/transformer_actions/column/sum.py"}, "transformers/transformer_actions/column/count.py:transformer:python:Total count of values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Total count of values in column", "path": "transformers/transformer_actions/column/count.py"}, "transformers/transformer_actions/column/clean_column_name.py:transformer:python:Clean column name:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Clean column name", "path": "transformers/transformer_actions/column/clean_column_name.py"}, "transformers/transformer_actions/column/fix_syntax_errors.py:transformer:python:Fix syntax errors:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Fix syntax errors", "path": "transformers/transformer_actions/column/fix_syntax_errors.py"}, "transformers/transformer_actions/column/reformat.py:transformer:python:Reformat values in column:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Reformat values in column", "path": "transformers/transformer_actions/column/reformat.py"}, "transformers/transformer_actions/column/select.py:transformer:python:Keep column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Keep column(s)", "path": "transformers/transformer_actions/column/select.py"}, "transformers/transformer_actions/column/remove.py:transformer:python:Remove column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Remove column(s)", "path": "transformers/transformer_actions/column/remove.py"}, "transformers/transformer_actions/column/shift_down.py:transformer:python:Shift row values down:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values down", "path": "transformers/transformer_actions/column/shift_down.py"}, "transformers/transformer_actions/column/shift_up.py:transformer:python:Shift row values up:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values up", "path": "transformers/transformer_actions/column/shift_up.py"}, "transformers/transformer_actions/column/normalize.py:transformer:python:Normalize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Normalize data", "path": "transformers/transformer_actions/column/normalize.py"}, "transformers/transformer_actions/column/standardize.py:transformer:python:Standardize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Standardize data", "path": "transformers/transformer_actions/column/standardize.py"}, "transformers/transformer_actions/column/impute.py:transformer:python:Fill in missing values:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Fill in missing values", "path": "transformers/transformer_actions/column/impute.py"}, "transformers/transformer_actions/column/remove_outliers.py:transformer:python:Remove outliers:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Remove outliers", "path": "transformers/transformer_actions/column/remove_outliers.py"}, "transformers/transformer_actions/column/diff.py:transformer:python:Calculate difference between values:Column actions:Feature extraction": {"block_type": "transformer", "groups": ["Column actions", "Feature extraction"], "language": "python", "name": "Calculate difference between values", "path": "transformers/transformer_actions/column/diff.py"}, "data_exporters/default.jinja:data_exporter:python:Base template (generic)": {"block_type": "data_exporter", "language": "python", "name": "Base template (generic)", "path": "data_exporters/default.jinja"}, "data_exporters/file.py:data_exporter:python:Local file": {"block_type": "data_exporter", "language": "python", "name": "Local file", "path": "data_exporters/file.py"}, "data_exporters/google_sheets.py:data_exporter:python:Google Sheets": {"block_type": "data_exporter", "language": "python", "name": "Google Sheets", "path": "data_exporters/google_sheets.py"}, "data_exporters/s3.py:data_exporter:python:Amazon S3:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_exporters/s3.py"}, "data_exporters/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/azure_blob_storage.py"}, "data_exporters/google_cloud_storage.py:data_exporter:python:Google Cloud Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/google_cloud_storage.py"}, "data_exporters/redshift.py:data_exporter:python:Amazon Redshift:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_exporters/redshift.py"}, "data_exporters/bigquery.py:data_exporter:python:Google BigQuery:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_exporters/bigquery.py"}, "data_exporters/snowflake.py:data_exporter:python:Snowflake:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_exporters/snowflake.py"}, "data_exporters/algolia.py:data_exporter:python:Algolia:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_exporters/algolia.py"}, "data_exporters/chroma.py:data_exporter:python:Chroma:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_exporters/chroma.py"}, "data_exporters/duckdb.py:data_exporter:python:DuckDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_exporters/duckdb.py"}, "data_exporters/mysql.py:data_exporter:python:MySQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_exporters/mysql.py"}, "data_exporters/oracledb.py:data_exporter:python:OracleDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "OracleDB", "path": "data_exporters/oracledb.py"}, "data_exporters/postgres.py:data_exporter:python:PostgreSQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_exporters/postgres.py"}, "data_exporters/qdrant.py:data_exporter:python:Qdrant:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_exporters/qdrant.py"}, "data_exporters/weaviate.py:data_exporter:python:Weaviate:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_exporters/weaviate.py"}, "sensors/default.py:sensor:python:Base template (generic)": {"block_type": "sensor", "language": "python", "name": "Base template (generic)", "path": "sensors/default.py"}, "sensors/s3.py:sensor:python:Amazon S3:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "sensors/s3.py"}, "sensors/google_cloud_storage.py:sensor:python:Google Cloud Storage:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "sensors/google_cloud_storage.py"}, "sensors/redshift.py:sensor:python:Amazon Redshift:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "sensors/redshift.py"}, "sensors/bigquery.py:sensor:python:Google BigQuery:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "sensors/bigquery.py"}, "sensors/snowflake.py:sensor:python:Snowflake:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "sensors/snowflake.py"}, "sensors/mysql.py:sensor:python:MySQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "sensors/mysql.py"}, "sensors/postgres.py:sensor:python:PostgreSQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "sensors/postgres.py"}}}